Federated Learning is an emerging distributed machine learning technique which does not require the transmission of data to a central server to build a global model. Instead, individual devices build their own models, and the model parameters are transmitted. The server constructs a global model using these parameters, which is then re-transmitted back to the devices. The major bottleneck of this approach is the communication overhead as all the devices need to transmit their model parameters at regular intervals. Here we present an interesting and novel alternative to federated learning known as Fusion Learning, where the distribution parameters of the clientâ€™s data along with its local model parameters are sent to the server. The server regenerates the data from these distribution parameters and fuses all the data from multiple devices. This combined dataset is now used to build a global model that is transmitted back to the individual devices. Our experiments show that the accuracy achieved through this approach is in par with both a federated setup and a centralized framework needing only one round of communication to the central server.